{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhOY5KOrzfiw"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBNCAElNEGG0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "import re\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4hWUQtPQHZy"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP87J6k2PxVd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJqYvXY8yz9q"
      },
      "source": [
        "Build service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJO-sTQ0skZU"
      },
      "outputs": [],
      "source": [
        "# Path to the uploaded service account key file in Colab\n",
        "SERVICE_ACCOUNT_FILE = '/content/drive/MyDrive/Colab Notebooks/Merchant Center API/service_account_key.json'\n",
        "\n",
        "# Define the required API scope\n",
        "SCOPES = ['https://www.googleapis.com/auth/content']\n",
        "\n",
        "# Authenticate using the service account\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    SERVICE_ACCOUNT_FILE, scopes=SCOPES\n",
        ")\n",
        "\n",
        "# Build the Google Content API client\n",
        "service = build('content', 'v2.1', credentials=credentials)\n",
        "\n",
        "# Merchant Center account ID (replace with your actual ID)\n",
        "merchant_id = '5411908926'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PZ8W0HQTEet"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIQYPngeyxnB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example: Get details of the Merchant Center account (non-MCA)\n",
        "try:\n",
        "    request = service.accounts().get(merchantId=merchant_id, accountId=merchant_id)\n",
        "    response = request.execute()\n",
        "    print(\"Account details:\")\n",
        "    print(json.dumps(response, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-ozq4Td0r1P7"
      },
      "outputs": [],
      "source": [
        "# List products in the Merchant Center account\n",
        "try:\n",
        "    request = service.products().list(merchantId=merchant_id)\n",
        "    response = request.execute()\n",
        "    print(\"Product list:\")\n",
        "    print(json.dumps(response, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-oN-yEtzJ4_"
      },
      "outputs": [],
      "source": [
        "# Example: Get details of the Merchant Center account (non-MCA)\n",
        "try:\n",
        "    # Use the same merchantId and accountId for non-MCA accounts\n",
        "    request = service.accounts().get(merchantId=merchant_id, accountId=merchant_id)\n",
        "    response = request.execute()\n",
        "    print(\"Account details:\")\n",
        "    print(json.dumps(response, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7R6ci1j0t25"
      },
      "outputs": [],
      "source": [
        "# Get shipping settings for the Merchant Center account\n",
        "try:\n",
        "    request = service.shippingsettings().get(merchantId=merchant_id, accountId=merchant_id)\n",
        "    response = request.execute()\n",
        "    print(\"Shipping settings:\")\n",
        "    print(json.dumps(response, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBopi9N912b4"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    request = service.accountstatuses().list(merchantId=merchant_id)\n",
        "    response = request.execute()\n",
        "    print(\"Account status:\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apipn2ERztaG"
      },
      "outputs": [],
      "source": [
        "product_id = 'online:en:US:09780062441720'\n",
        "product_id2 = '09798886633368'\n",
        "\n",
        "\n",
        "try:\n",
        "    # Retrieve product data\n",
        "    request = service.products().get(merchantId=merchant_id, productId=product_id)\n",
        "    response = request.execute()\n",
        "    print(\"Product details:\")\n",
        "    print(json.dumps(response, indent=4))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npWI53senfvL"
      },
      "source": [
        "\n",
        "# Scraping Publicaiton Pages (front end) from a US IP and updating the pricing for single issue product\n",
        "\n",
        "1. Prepare your CSV (e.g., merchant_center_products.csv) with the required columns.\n",
        "2. Update CSV_FILE_PATH if it’s different.\n",
        "3. Run script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACLkROVHOGD0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "import re\n",
        "\n",
        "\n",
        "# Path to service account key (update as needed)\n",
        "SERVICE_ACCOUNT_FILE = '/content/drive/MyDrive/Colab Notebooks/Merchant Center API/service_account_key.json'\n",
        "\n",
        "# API scope\n",
        "SCOPES = ['https://www.googleapis.com/auth/content']\n",
        "\n",
        "# Authenticate using the service account\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    SERVICE_ACCOUNT_FILE, scopes=SCOPES\n",
        ")\n",
        "\n",
        "# Build the Google Content API client\n",
        "service = build('content', 'v2.1', credentials=credentials)\n",
        "\n",
        "# Merchant Center account ID (update if needed)\n",
        "MERCHANT_ID = '5411908926'\n",
        "\n",
        "# Read CSV with product data\n",
        "csv_file_path = '/content/products.csv'  # Update path if needed\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Headers for web scraping\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\",\n",
        "    \"X-Forwarded-For\": \"34.85.12.34\"  # Simulated US IP\n",
        "}\n",
        "\n",
        "# Needs feed label, language, ID\n",
        "def build_product_id(row):\n",
        "    \"\"\"Construct the correct product ID format\"\"\"\n",
        "    return f\"online:{row['language']}:{row['feed label']}:{row['id']}\"\n",
        "\n",
        "def get_product_info(product_id):\n",
        "    \"\"\"Fetch product details from Merchant Center API\"\"\"\n",
        "    try:\n",
        "        request = service.products().get(merchantId=MERCHANT_ID, productId=product_id)\n",
        "        product_data = request.execute()\n",
        "        return product_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching product {product_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_price(url):\n",
        "    \"\"\"Scrape the correct price from the product page and extract a valid float.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the price element (update selector if needed)\n",
        "        price_element = soup.select_one(\"[data-testid='singleIssueProduct'] .btn-title\")\n",
        "        if price_element:\n",
        "            price_text = price_element.text.strip()\n",
        "\n",
        "            # Extract numbers and decimals (handles both '.' and ',')\n",
        "            price_numbers = re.findall(r'\\d+[\\.,]?\\d*', price_text)\n",
        "            if price_numbers:\n",
        "                # Replace comma with dot for float conversion (handles '1,99' → '1.99')\n",
        "                clean_price = price_numbers[0].replace(\",\", \".\")\n",
        "                return float(clean_price)\n",
        "\n",
        "        print(f\"Skipping {url}: No valid price found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def update_product_price(product_id, new_price):\n",
        "    \"\"\"Update the product price in Merchant Center\"\"\"\n",
        "    try:\n",
        "        product_data = get_product_info(product_id)\n",
        "        if not product_data:\n",
        "            return\n",
        "\n",
        "        updated_product = {\n",
        "            \"price\": {\n",
        "                \"value\": str(new_price),\n",
        "                \"currency\": \"USD\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        request = service.products().update(\n",
        "            merchantId=MERCHANT_ID,\n",
        "            productId=product_id,\n",
        "            body=updated_product\n",
        "        )\n",
        "        response = request.execute()\n",
        "        print(f\"Updated {product_id} to ${new_price}\")\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating product {product_id}: {e}\")\n",
        "\n",
        "# Process each product\n",
        "for _, row in df.iterrows():\n",
        "    product_id = build_product_id(row)\n",
        "    product_info = get_product_info(product_id)\n",
        "\n",
        "    if product_info and 'link' in product_info:\n",
        "        product_url = product_info['link']\n",
        "        correct_price = extract_price(product_url)\n",
        "        if correct_price:\n",
        "            update_product_price(product_id, correct_price)\n",
        "        else:\n",
        "            print(f\"Skipping update for {product_id}: Price not found.\")\n",
        "    else:\n",
        "        print(f\"Skipping {product_id}: No product link found.\")\n",
        "\n",
        "print(\"✅ Price updates completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgGk9o21VQeY"
      },
      "source": [
        "# Scraping Publicaiton Pages (schema) from a US IP and updating the images for single issue product\n",
        "\n",
        "1. Prepare your CSV (e.g., merchant_center_products.csv) with the required columns.\n",
        "2. Update CSV_FILE_PATH if it’s different.\n",
        "3. Run script\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4mEBvFVUVQ_z",
        "outputId": "52138c5d-0472-4ac6-fc83-a018346e3916"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Google Merchant Center API Setup\n",
        "SERVICE_ACCOUNT_FILE = \"/content/drive/MyDrive/Colab Notebooks/Merchant Center API/service_account_key.json\"\n",
        "SCOPES = [\"https://www.googleapis.com/auth/content\"]\n",
        "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "service = build(\"content\", \"v2.1\", credentials=credentials)\n",
        "\n",
        "# Your Merchant Center ID\n",
        "MERCHANT_ID = \"5411908926\"\n",
        "\n",
        "# Headers for scraping\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\"\n",
        "}\n",
        "\n",
        "# CSV file with products\n",
        "CSV_FILE_PATH = \"/content/11.csv\"  # Update this path as needed\n",
        "\n",
        "# Your provided functions\n",
        "def build_product_id(row):\n",
        "    \"\"\"Construct the correct product ID format\"\"\"\n",
        "    return f\"online:{row['language']}:{row['feed label']}:{row['id']}\"\n",
        "\n",
        "def get_product_info(product_id):\n",
        "    \"\"\"Fetch product details from Merchant Center API\"\"\"\n",
        "    try:\n",
        "        request = service.products().get(merchantId=MERCHANT_ID, productId=product_id)\n",
        "        product_data = request.execute()\n",
        "        return product_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching product {product_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract schema.org JSON-LD, selecting only Product type\n",
        "def extract_schema(url):\n",
        "    print(f\"Step 1: Scraping URL: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\", from_encoding=\"utf-8\")\n",
        "        script_tags = soup.find_all(\"script\", type=\"application/ld+json\")\n",
        "        if not script_tags:\n",
        "            print(\"Step 2: No JSON-LD scripts found\")\n",
        "            return None\n",
        "\n",
        "        for i, script in enumerate(script_tags, 1):\n",
        "            try:\n",
        "                script_text = script.string.encode().decode('utf-8')\n",
        "                data = json.loads(script_text)\n",
        "                print(f\"Step 2: Found schema {i}: {json.dumps(data, indent=2, ensure_ascii=False)}\")\n",
        "                if data.get(\"@type\") == \"Product\":\n",
        "                    print(f\"Step 2: Selected Product schema {i}\")\n",
        "                    return data\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Step 2: Schema {i} is invalid JSON\")\n",
        "        print(\"Step 2: No Product schema found among scripts\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Step 2: Could not scrape {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Get the updated image from schema\n",
        "def get_updated_image(schema):\n",
        "    if not schema or schema.get(\"@type\") != \"Product\":\n",
        "        print(\"Step 3: No valid Product schema for image update\")\n",
        "        return None\n",
        "    image = schema.get(\"image\", None)\n",
        "    if image:\n",
        "        print(f\"Step 3: Updated image found: {image}\")\n",
        "        return image\n",
        "    print(\"Step 3: No image found in schema\")\n",
        "    return None\n",
        "\n",
        "# Update product image in Merchant Center\n",
        "def update_product_image(product_id, new_image):\n",
        "    print(f\"Step 4: Updating image for product {product_id}\")\n",
        "    try:\n",
        "        # Fetch current product data\n",
        "        product_data = get_product_info(product_id)\n",
        "        if not product_data:\n",
        "            print(f\"Step 4: Skipping update - could not fetch product {product_id}\")\n",
        "            return\n",
        "\n",
        "        # Update only the imageLink field\n",
        "        updated_product = {\n",
        "            \"imageLink\": new_image\n",
        "        }\n",
        "        request = service.products().update(\n",
        "            merchantId=MERCHANT_ID,\n",
        "            productId=product_id,\n",
        "            body=updated_product,\n",
        "            updateMask=\"imageLink\"  # Only update the imageLink field\n",
        "        )\n",
        "        request.execute()\n",
        "        print(f\"Step 5: Successfully updated image for {product_id} to {new_image}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Step 5: Failed to update image for {product_id}: {e}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Read CSV\n",
        "    df = pd.read_csv(CSV_FILE_PATH)\n",
        "    print(f\"Loaded {len(df)} products from CSV\")\n",
        "\n",
        "    print(\"\\nStarting product updates...\\n\")\n",
        "    for i, row in df.iterrows():\n",
        "        print(f\"--- Processing Product {i+1} of {len(df)} ---\")\n",
        "\n",
        "        # Build product ID\n",
        "        product_id = build_product_id(row)\n",
        "        print(f\"Product ID: {product_id}\")\n",
        "\n",
        "        # Get URL from CSV (assuming 'link' column exists)\n",
        "        url = row.get(\"link\")\n",
        "        if not url:\n",
        "            print(\"No URL found in CSV row, skipping\")\n",
        "            continue\n",
        "\n",
        "        # Scrape schema and get new image\n",
        "        schema = extract_schema(url)\n",
        "        new_image = get_updated_image(schema)\n",
        "\n",
        "        # Update product if new image is found\n",
        "        if new_image:\n",
        "            update_product_image(product_id, new_image)\n",
        "        else:\n",
        "            print(f\"No new image to update for {product_id}\")\n",
        "\n",
        "        print(f\"--- Finished Product {i+1} ---\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdRcgTwLvFE"
      },
      "source": [
        "# Extracting Product Schema and Uploading it to a new data source on GMC (Single Issue)\n",
        "\n",
        "1. Upload CSV with URLs\n",
        "2. Change target country, language\n",
        "3. Change Currency to choose desired schema offer\n",
        "4. Run *script*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4jA3EGOGM8UR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Google Merchant Center API Setup\n",
        "SERVICE_ACCOUNT_FILE = \"/content/drive/MyDrive/Colab Notebooks/Merchant Center API/service_account_key.json\"\n",
        "SCOPES = [\"https://www.googleapis.com/auth/content\"]\n",
        "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "service = build(\"content\", \"v2.1\", credentials=credentials)\n",
        "\n",
        "# Your Merchant Center ID\n",
        "merchant_id = \"5411908926\"\n",
        "\n",
        "# Headers for scraping\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\"\n",
        "}\n",
        "\n",
        "# CSV file with URLs\n",
        "CSV_FILE_PATH = \"/content/Newspapers-es.csv\"\n",
        "#df = pd.read_csv(CSV_FILE_PATH, skiprows=1611)\n",
        "\n",
        "\n",
        "# Target country, language, and publication type\n",
        "target_country = \"ES\"\n",
        "target_language = \"es\"\n",
        "publication_type = \"Newspaper\" # Newspaper or Magazine\n",
        "\n",
        "# Extract schema.org JSON-LD, selecting only Product type\n",
        "def extract_schema(url):\n",
        "    print(f\"Step 1: Scraping URL: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\", from_encoding=\"utf-8\")\n",
        "        script_tags = soup.find_all(\"script\", type=\"application/ld+json\")\n",
        "        if not script_tags:\n",
        "            print(\"Step 2: No JSON-LD scripts found\")\n",
        "            return None\n",
        "\n",
        "        for i, script in enumerate(script_tags, 1):\n",
        "            try:\n",
        "                script_text = script.string.encode().decode('utf-8')\n",
        "                data = json.loads(script_text)\n",
        "                print(f\"Step 2: Found schema {i}: {json.dumps(data, indent=2, ensure_ascii=False)}\")\n",
        "                if data.get(\"@type\") == \"Product\":\n",
        "                    print(f\"Step 2: Selected Product schema {i}\")\n",
        "                    return data\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Step 2: Schema {i} is invalid JSON\")\n",
        "        print(\"Step 2: No Product schema found among scripts\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Step 2: Could not scrape {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process schema to get product data (first single-issue offer in target currency)\n",
        "def process_product_data(schema, target_currency):\n",
        "    if not schema:\n",
        "        print(\"Step 3: No schema provided\")\n",
        "        return None\n",
        "    print(f\"Step 3: Processing schema: {json.dumps(schema, indent=2, ensure_ascii=False)}\")\n",
        "\n",
        "    if schema.get(\"@type\") != \"Product\":\n",
        "        print(f\"Step 3: Invalid type: {schema.get('@type')}\")\n",
        "        return None\n",
        "\n",
        "    offers = schema.get(\"offers\", [])\n",
        "    if isinstance(offers, dict):\n",
        "        offers = [offers]\n",
        "    print(f\"Step 3: Offers: {offers}\")\n",
        "\n",
        "    # Pick the first single-issue offer in the target currency\n",
        "    for offer in offers:\n",
        "        offer_name = offer.get(\"name\", \"\")\n",
        "        offer_currency = offer.get(\"priceCurrency\", \"\")\n",
        "        offer_type = offer.get(\"offerType\", \"\")  # Check for subscription\n",
        "        print(f\"Step 3: Checking offer - Name: {offer_name}, Currency: {offer_currency}, Type: {offer_type}\")\n",
        "        if offer_currency == target_currency and offer_type != \"Subscription\":\n",
        "            price = offer.get(\"price\")\n",
        "            # Skip if price is zero\n",
        "            if str(price) in [\"0\", \"0.0\", \"0.00\"]:\n",
        "                print(f\"Step 3: Skipping product '{offer_name}' - Price is zero\")\n",
        "                return None\n",
        "            # Proceed with non-zero price\n",
        "            url_parts = offer.get(\"url\", \"\").split(\"/\")\n",
        "            last_segment = url_parts[-1]  # e.g., \"the-boston-globe\"\n",
        "            locale_code = \"\"\n",
        "            content_types = [\"newspapers\", \"magazines\"]\n",
        "            for i, part in enumerate(url_parts):\n",
        "                if \"pressreader.com\" in url_parts[i-1] and part not in content_types and i+1 < len(url_parts) and url_parts[i+1] in content_types:\n",
        "                    locale_code = part\n",
        "                    break\n",
        "            offer_id = f\"{locale_code}-{last_segment}\" if locale_code else last_segment\n",
        "            product_data = {\n",
        "                \"offerId\": offer_id,\n",
        "                \"title\": offer_name,\n",
        "                \"description\": schema.get(\"description\", \"\"),\n",
        "                \"imageLink\": schema.get(\"image\", \"https://via.placeholder.com/150\"),\n",
        "                \"link\": offer.get(\"url\"),\n",
        "                \"contentLanguage\": target_language,\n",
        "                \"targetCountry\": target_country,\n",
        "                \"channel\": \"online\",\n",
        "                \"availability\": \"in stock\",\n",
        "                \"price\": {\"value\": str(price), \"currency\": offer_currency},\n",
        "                \"customLabel0\": f\"{publication_type} - Single Issue - {target_language.upper()}\"\n",
        "            }\n",
        "            print(f\"Step 3: Processed product data - Title: {product_data['title']}, Custom Label: {product_data['customLabel0']}\")\n",
        "            return product_data\n",
        "    print(f\"Step 3: No {target_currency} single-issue offer found\")\n",
        "    return None\n",
        "\n",
        "# Upload product to Google Merchant Center\n",
        "def upload_product(product_data):\n",
        "    print(f\"Step 4: Starting upload for {product_data['title']}\")\n",
        "    try:\n",
        "        service.products().insert(merchantId=merchant_id, body=product_data).execute()\n",
        "        print(f\"Step 5: Successfully uploaded: {product_data['title']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Step 5: Failed to upload {product_data['title']}: {e}\")\n",
        "\n",
        "# Create a feed for the target country\n",
        "def create_data_source():\n",
        "    feed_name = \"PressReader Feed\"\n",
        "    print(\"Creating feed...\")\n",
        "    try:\n",
        "        existing_feeds = service.datafeeds().list(merchantId=merchant_id).execute()\n",
        "        if any(feed[\"name\"] == feed_name for feed in existing_feeds.get(\"resources\", [])):\n",
        "            print(f\"Feed '{feed_name}' already exists\")\n",
        "            return\n",
        "\n",
        "        service.datafeeds().insert(\n",
        "            merchantId=merchant_id,\n",
        "            body={\n",
        "                \"name\": feed_name,\n",
        "                \"contentType\": \"products\",\n",
        "                \"fileName\": f\"{publication_type} - Single Issue - {target_country}.csv\",\n",
        "                \"targets\": [\n",
        "                    {\"country\": target_country, \"language\": target_language, \"includedDestinations\": [\"Shopping\"]}\n",
        "                ]\n",
        "            }\n",
        "        ).execute()\n",
        "        print(f\"Created feed: {feed_name} targeting {target_country}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create feed: {e}\")\n",
        "\n",
        "# Main function with required currency parameter\n",
        "def main(target_currency):\n",
        "    df = pd.read_csv(CSV_FILE_PATH)\n",
        "    create_data_source()\n",
        "\n",
        "    print(f\"\\nStarting URL processing with target currency: {target_currency}...\\n\")\n",
        "    for i, url in enumerate(df[\"url\"], 1):\n",
        "        print(f\"--- Processing URL {i} of {len(df['url'])} ---\")\n",
        "        schema = extract_schema(url)\n",
        "        product_data = process_product_data(schema, target_currency)\n",
        "        if product_data:\n",
        "            upload_product(product_data)\n",
        "        print(f\"--- Finished URL {i} ---\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify your desired currency here (required)\n",
        "    main(\"USD\")  # Must provide a currency, e.g., \"EUR\", \"GBP\", \"USD\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSwEQwq80xts"
      },
      "source": [
        "# Extracting Product Schema and Uploading it to a new data source on GMC (Subscription)\n",
        "\n",
        "1. Upload CSV with URLs\n",
        "2. Change target country, language\n",
        "3. Change Currency to choose desired schema offer\n",
        "4. Run *script*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GSwLrNrRjWz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "import time\n",
        "\n",
        "# Google Merchant Center API Setup\n",
        "SERVICE_ACCOUNT_FILE = \"/content/drive/MyDrive/Colab Notebooks/Merchant Center API/service_account_key.json\"\n",
        "SCOPES = [\"https://www.googleapis.com/auth/content\"]\n",
        "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "service = build(\"content\", \"v2.1\", credentials=credentials)\n",
        "\n",
        "merchant_id = \"5411908926\"\n",
        "HEADERS = {\"User-Agent\": \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\"}\n",
        "CSV_FILE_PATH = \"/content/Newspapers-es.csv\"\n",
        "\n",
        "# Target country and language\n",
        "target_country = \"ES\"\n",
        "target_language = \"es\"\n",
        "publication_type = \"Newspaper\"\n",
        "\n",
        "def extract_schema(url):\n",
        "    print(f\"Step 1: Scraping URL: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\", from_encoding=\"utf-8\")\n",
        "        script_tags = soup.find_all(\"script\", type=\"application/ld+json\")\n",
        "        if not script_tags:\n",
        "            print(\"Step 2: No JSON-LD scripts found\")\n",
        "            return None\n",
        "\n",
        "        for i, script in enumerate(script_tags, 1):\n",
        "            try:\n",
        "                script_text = script.string.encode().decode('utf-8')\n",
        "                data = json.loads(script_text)\n",
        "                print(f\"Step 2: Found schema {i}: {json.dumps(data, indent=2, ensure_ascii=False)}\")\n",
        "                if data.get(\"@type\") == \"Product\":\n",
        "                    print(f\"Step 2: Selected Product schema {i}\")\n",
        "                    return data\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Step 2: Schema {i} is invalid JSON\")\n",
        "        print(\"Step 2: No Product schema found among scripts\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Step 2: Could not scrape {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process schema to get product data (first subscription offer in target currency)\n",
        "def process_product_data(schema, target_currency):\n",
        "    if not schema:\n",
        "        print(\"Step 3: No schema provided\")\n",
        "        return None\n",
        "    print(f\"Step 3: Processing schema: {json.dumps(schema, indent=2, ensure_ascii=False)}\")\n",
        "\n",
        "    if schema.get(\"@type\") != \"Product\":\n",
        "        print(f\"Step 3: Invalid type: {schema.get('@type')}\")\n",
        "        return None\n",
        "\n",
        "    offers = schema.get(\"offers\", [])\n",
        "    if isinstance(offers, dict):\n",
        "        offers = [offers]\n",
        "    print(f\"Step 3: Offers: {offers}\")\n",
        "\n",
        "    # Pick the first subscription offer in the target currency\n",
        "    for offer in offers:\n",
        "        offer_name = offer.get(\"name\", \"\")\n",
        "        offer_currency = offer.get(\"priceCurrency\", \"\")\n",
        "        offer_type = offer.get(\"offerType\", \"\")\n",
        "        print(f\"Step 3: Checking offer - Name: {offer_name}, Currency: {offer_currency}, Type: {offer_type}\")\n",
        "        if offer_currency == target_currency and offer_type == \"Subscription\":\n",
        "            url_parts = offer.get(\"url\", \"\").split(\"/\")\n",
        "            last_segment = url_parts[-1]\n",
        "            locale_code = \"\"\n",
        "            content_types = [\"newspapers\", \"magazines\"]\n",
        "            for i, part in enumerate(url_parts):\n",
        "                if \"pressreader.com\" in url_parts[i-1] and part not in content_types and i+1 < len(url_parts) and url_parts[i+1] in content_types:\n",
        "                    locale_code = part\n",
        "                    break\n",
        "            offer_id = f\"{locale_code}-{last_segment}-sub\" if locale_code else f\"{last_segment}-sub\"\n",
        "            product_data = {\n",
        "                \"offerId\": offer_id,\n",
        "                \"title\": offer_name,\n",
        "                \"description\": schema.get(\"description\", \"\"),\n",
        "                \"imageLink\": schema.get(\"image\", \"https://via.placeholder.com/150\"),\n",
        "                \"link\": offer.get(\"url\"),\n",
        "                \"contentLanguage\": target_language,\n",
        "                \"targetCountry\": target_country,\n",
        "                \"channel\": \"online\",\n",
        "                \"availability\": \"in stock\",\n",
        "                \"price\": {\"value\": str(offer.get(\"price\")), \"currency\": offer_currency},\n",
        "                \"customLabel0\": f\"{publication_type} - Subscription - {target_language.upper()}\"\n",
        "            }\n",
        "            print(f\"Step 3: Processed product data - Title: {product_data['title']}\")\n",
        "            return product_data\n",
        "    print(f\"Step 3: No {target_currency} subscription offer found\")\n",
        "    return None\n",
        "\n",
        "def upload_product(product_data):\n",
        "    print(f\"Step 4: Starting upload for {product_data['title']}\")\n",
        "    try:\n",
        "        service.products().insert(merchantId=merchant_id, body=product_data).execute()\n",
        "        print(f\"Step 5: Successfully uploaded: {product_data['title']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Step 5: Failed to upload {product_data['title']}: {e}\")\n",
        "\n",
        "def create_data_source():\n",
        "    feed_name = \"PressReader Feed 2\"\n",
        "    print(\"Creating feed...\")\n",
        "    try:\n",
        "        existing_feeds = service.datafeeds().list(merchantId=merchant_id).execute()\n",
        "        if any(feed[\"name\"] == feed_name for feed in existing_feeds.get(\"resources\", [])):\n",
        "            print(f\"Feed '{feed_name}' already exists\")\n",
        "            return\n",
        "\n",
        "        service.datafeeds().insert(\n",
        "            merchantId=merchant_id,\n",
        "            body={\n",
        "                \"name\": feed_name,\n",
        "                \"contentType\": \"products\",\n",
        "                \"fileName\": f\"{publication_type} - Subscription - {target_country}.csv\",\n",
        "                \"targets\": [\n",
        "                    {\"country\": target_country, \"language\": target_language, \"includedDestinations\": [\"Shopping\"]}\n",
        "                ]\n",
        "            }\n",
        "        ).execute()\n",
        "        print(f\"Created feed: {feed_name} targeting {target_country}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create feed: {e}\")\n",
        "\n",
        "def main(target_currency):\n",
        "    df = pd.read_csv(CSV_FILE_PATH)\n",
        "    create_data_source()\n",
        "\n",
        "    print(f\"\\nStarting URL processing with target currency: {target_currency}...\\n\")\n",
        "    for i, url in enumerate(df[\"url\"], 1):\n",
        "        print(f\"--- Processing URL {i} of {len(df['url'])} ---\")\n",
        "        schema = extract_schema(url)\n",
        "        product_data = process_product_data(schema, target_currency)\n",
        "        if product_data:\n",
        "            upload_product(product_data)\n",
        "        print(f\"--- Finished URL {i} ---\\n\")\n",
        "        time.sleep(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify your desired currency here (required)\n",
        "    main(\"USD\")  # Must provide a currency, e.g., \"EUR\", \"GBP\", \"USD\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWXKKNL02Cm1"
      },
      "source": [
        "# Delete Products\n",
        "\n",
        "1. Upload CSV with required columns\n",
        "2. Run script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSm1L0192CJN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Google Merchant Center API Setup\n",
        "SERVICE_ACCOUNT_FILE = \"/content/drive/MyDrive/Colab Notebooks/Merchant Center API/service_account_key.json\"\n",
        "SCOPES = [\"https://www.googleapis.com/auth/content\"]\n",
        "MERCHANT_ID = \"5411908926\"\n",
        "\n",
        "# CSV file with products to delete\n",
        "CSV_FILE_PATH = \"/content/delete.csv\"  # Update this path\n",
        "\n",
        "# Authenticate and build the service\n",
        "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "service = build(\"content\", \"v2.1\", credentials=credentials)\n",
        "\n",
        "# Your provided functions\n",
        "def build_product_id(row):\n",
        "    \"\"\"Construct the correct product ID format\"\"\"\n",
        "    return f\"online:{row['language']}:{row['feed label']}:{row['id']}\"\n",
        "\n",
        "def get_product_info(product_id):\n",
        "    \"\"\"Fetch product details from Merchant Center API\"\"\"\n",
        "    try:\n",
        "        request = service.products().get(merchantId=MERCHANT_ID, productId=product_id)\n",
        "        product_data = request.execute()\n",
        "        return product_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching product {product_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Delete a product by productId\n",
        "def delete_product(product_id):\n",
        "    try:\n",
        "        service.products().delete(merchantId=MERCHANT_ID, productId=product_id).execute()\n",
        "        print(f\"Successfully deleted product: {product_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to delete product {product_id}: {e}\")\n",
        "\n",
        "# Main function to process CSV and delete products\n",
        "def main():\n",
        "    # Read the CSV\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_FILE_PATH)\n",
        "        print(f\"Loaded CSV with {len(df)} products to delete\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    required_columns = ['language', 'feed label', 'id']\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        print(f\"CSV must contain these columns: {required_columns}\")\n",
        "        return\n",
        "\n",
        "    # Process each row\n",
        "    for index, row in df.iterrows():\n",
        "        product_id = build_product_id(row)\n",
        "        print(f\"\\nProcessing row {index + 1} of {len(df)}: {product_id}\")\n",
        "\n",
        "        # Optional: Verify product exists (comment out if not needed)\n",
        "        product_info = get_product_info(product_id)\n",
        "        if product_info:\n",
        "            print(f\"Found product: {product_info.get('title', 'Unknown title')}\")\n",
        "        else:\n",
        "            print(f\"Product not found or error, attempting deletion anyway\")\n",
        "\n",
        "        # Delete the product\n",
        "        delete_product(product_id)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmXAuB6VWlZj"
      },
      "source": [
        "# Extracting Product Schema and Uploading it to a new data source on GMC (Books)\n",
        "\n",
        "1. Upload CSV with URLs\n",
        "2. Change target country, language\n",
        "3. Change region between CA and US to get CAD or USD Currency in schema\n",
        "4. Run *script*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6EyXHwEWwC0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Google Merchant Center API Setup\n",
        "SERVICE_ACCOUNT_FILE = \"/content/drive/MyDrive/Colab Notebooks/Merchant Center API/service_account_key.json\"\n",
        "SCOPES = [\"https://www.googleapis.com/auth/content\"]\n",
        "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "service = build(\"content\", \"v2.1\", credentials=credentials)\n",
        "\n",
        "merchant_id = \"5411908926\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; Naverbot/1.0; +http://naver.com/bot)\"}\n",
        "CSV_FILE_PATH = \"/content/Book1.csv\"\n",
        "\n",
        "# Target country, language, and publication type (for Merchant Center)\n",
        "target_country = \"CA\"\n",
        "target_language = \"en\"\n",
        "publication_type = \"Book\"\n",
        "\n",
        "# US IP for explicit US region requests\n",
        "US_IP = \"104.131.0.1\"  # Example US IP (e.g., New York)\n",
        "\n",
        "def extract_schema(url, region):\n",
        "    print(f\"Step 1: Scraping URL: {url} from region: {region}\")\n",
        "    try:\n",
        "        # Set headers\n",
        "        headers = HEADERS.copy()\n",
        "        headers[\"Accept-Language\"] = \"en-US\" if region == \"US\" else \"en-CA\"\n",
        "        if region == \"US\":\n",
        "            headers[\"X-Forwarded-For\"] = US_IP  # Only set US IP for explicit US region\n",
        "        print(f\"Headers: {headers}\")  # Log headers for debugging\n",
        "        session = requests.Session()  # Use fresh session to avoid cookie persistence\n",
        "        response = session.get(url, headers=headers, timeout=10)\n",
        "        response.encoding = 'utf-8'\n",
        "        print(f\"Response URL: {response.url}\")  # Log response URL\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\", from_encoding=\"utf-8\")\n",
        "        script_tags = soup.find_all(\"script\", type=\"application/ld+json\")\n",
        "        if not script_tags:\n",
        "            print(\"Step 2: No JSON-LD scripts found\")\n",
        "            return None\n",
        "\n",
        "        for i, script in enumerate(script_tags, 1):\n",
        "            try:\n",
        "                script_text = script.string.encode().decode('utf-8')\n",
        "                data = json.loads(script_text)\n",
        "                print(f\"Step 2: Found schema {i}: {json.dumps(data, indent=2, ensure_ascii=False)}\")\n",
        "                if data.get(\"@type\") == \"Product\":\n",
        "                    print(f\"Step 2: Selected Product schema {i}\")\n",
        "                    return data\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Step 2: Schema {i} is invalid JSON\")\n",
        "        print(\"Step 2: No Product schema found\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Step 2: Could not scrape {url} from {region}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_product_data(schema):\n",
        "    if not schema:\n",
        "        print(\"Step 3: No schema provided\")\n",
        "        return None\n",
        "    print(f\"Step 3: Processing schema: {json.dumps(schema, indent=2, ensure_ascii=False)}\")\n",
        "\n",
        "    if schema.get(\"@type\") != \"Product\":\n",
        "        print(f\"Step 3: Invalid type: {schema.get('@type')}, expected Product\")\n",
        "        return None\n",
        "\n",
        "    offers = schema.get(\"offers\", {})\n",
        "    if not isinstance(offers, dict):\n",
        "        print(\"Step 3: Invalid offers format for Product schema\")\n",
        "        return None\n",
        "\n",
        "    offer = offers\n",
        "    title = schema.get(\"name\", \"\")\n",
        "    description = schema.get(\"description\", \"No description available\")\n",
        "    image_link = schema.get(\"image\", \"https://via.placeholder.com/150\")\n",
        "    gtin = schema.get(\"gtin13\", \"\")\n",
        "    brand = schema.get(\"brand\", {}).get(\"name\", \"\")\n",
        "    url = offer.get(\"url\", \"\")\n",
        "\n",
        "    offer_currency = offer.get(\"priceCurrency\", \"\")\n",
        "    price = offer.get(\"price\")\n",
        "    print(f\"Step 3: Processing offer - Name: {title}, Currency: {offer_currency}, Price: {price}\")\n",
        "\n",
        "    if str(price) in [\"0\", \"0.0\", \"0.00\"]:\n",
        "        print(f\"Step 3: Skipping product '{title}' - Price is zero\")\n",
        "        return None\n",
        "\n",
        "    url_parts = url.split(\"/\")\n",
        "    last_segment = url_parts[-1]\n",
        "    locale_code = \"\"\n",
        "    content_types = [\"books\"]\n",
        "    for i, part in enumerate(url_parts):\n",
        "        if \"pressreader.com\" in url_parts[i-1] and part not in content_types and i+1 < len(url_parts) and url_parts[i+1] in content_types:\n",
        "            locale_code = part\n",
        "            break\n",
        "    offer_id = f\"{locale_code}-{last_segment}\" if locale_code else last_segment\n",
        "    product_data = {\n",
        "        \"offerId\": offer_id,\n",
        "        \"title\": title,\n",
        "        \"description\": description,\n",
        "        \"imageLink\": image_link,\n",
        "        \"link\": url,\n",
        "        \"contentLanguage\": target_language,\n",
        "        \"targetCountry\": target_country,\n",
        "        \"channel\": \"online\",\n",
        "        \"availability\": \"in stock\",\n",
        "        \"price\": {\"value\": str(price), \"currency\": offer_currency},\n",
        "        \"gtin\": gtin,\n",
        "        \"brand\": brand,\n",
        "        \"customLabel0\": f\"{publication_type} - {target_language.upper()}\"\n",
        "    }\n",
        "    print(f\"Step 3: Processed product data - Title: {product_data['title']}, Currency: {offer_currency}\")\n",
        "    return product_data\n",
        "\n",
        "def upload_product(product_data):\n",
        "    print(f\"Step 4: Starting upload for {product_data['title']}\")\n",
        "    try:\n",
        "        service.products().insert(merchantId=merchant_id, body=product_data).execute()\n",
        "        print(f\"Step 5: Successfully uploaded: {product_data['title']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Step 5: Failed to upload {product_data['title']}: {e}\")\n",
        "\n",
        "def main(region):\n",
        "    df = pd.read_csv(CSV_FILE_PATH)\n",
        "\n",
        "    print(f\"\\nStarting URL processing from region: {region}...\\n\")\n",
        "    for i, url in enumerate(df[\"url\"], 1):\n",
        "        print(f\"--- Processing URL {i} of {len(df['url'])} ---\")\n",
        "        schema = extract_schema(url, region)\n",
        "        product_data = process_product_data(schema)\n",
        "        if product_data:\n",
        "            upload_product(product_data)\n",
        "        print(f\"--- Finished URL {i} ---\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"CA\")  # Use default Canadian IP for CA"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1PZ8W0HQTEet",
        "npWI53senfvL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
